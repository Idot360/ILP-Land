{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__sigma_0 is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\evanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([653.08792288, 592.16905327]), array([316.68016218, 316.65121679]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n",
    "kernel = DotProduct() + WhiteKernel()\n",
    "gpr = GaussianProcessRegressor(kernel=kernel,\n",
    "        random_state=0).fit(X, y)\n",
    "gpr.score(X, y)\n",
    "gpr.predict(X[:2,:], return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [70 70 65 80 85 70 70 70 80 80 80]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LinearDiscriminantAnalysis was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import pandas as pd\n",
    "avo_TRAIN = pd.read_csv('dataset(small).csv', index_col='ID')\n",
    "# View the data!\n",
    "avo_TRAIN.head()\n",
    "#avo_TRAIN.describe()\n",
    "\n",
    "# Clean Data\n",
    "\n",
    "avo_TRAIN = avo_TRAIN.dropna(axis=0)\n",
    "\n",
    "features = [\"Population\",\"Land size\",\"Working hours\",\"Corruption\",\"Environment\",\"Income\",\"Cost of living\",\"N recreational buildings\",\"N office buildings\",\"N essential buildings\",\"N residential buildings\"]\n",
    "\n",
    "X = avo_TRAIN[features]\n",
    "y = avo_TRAIN.Happiness\n",
    "from sklearn import *\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X.to_numpy(), y.to_numpy())\n",
    "\n",
    "TEST = pd.read_csv('dataset(test).csv', index_col='ID')\n",
    "\n",
    "XforPredictions = TEST[features]\n",
    "# Make Predictions!\n",
    "yPredictions = clf.predict(XforPredictions)\n",
    "print(\"Predictions:\", yPredictions)\n",
    "# Save to CSV File!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training split:  9 ; test split:  2\n",
      "Predictions: [71.075 70.95  69.275 79.875 81.55  71.075 70.8   76.175 79.45  78.1\n",
      " 78.375]\n"
     ]
    }
   ],
   "source": [
    "# load csvs\n",
    "import pandas as pd\n",
    "avo_TRAIN = pd.read_csv('dataset(small).csv', index_col='ID')\n",
    "# View the data!\n",
    "avo_TRAIN.head()\n",
    "#avo_TRAIN.describe()\n",
    "\n",
    "# Clean Data\n",
    "\n",
    "# Drops missing values \n",
    "avo_TRAIN = avo_TRAIN.dropna(axis=0)\n",
    "\n",
    "\n",
    "# Read more advanced ways to clean data\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "# https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn\n",
    "# Select data for learning\n",
    "features = [\"Population\",\"Land size\",\"Working hours\",\"Corruption\",\"Environment\",\"Income\",\"Cost of living\",\"N recreational buildings\",\"N office buildings\",\"N essential buildings\",\"N residential buildings\"]\n",
    "\n",
    "X = avo_TRAIN[features]\n",
    "y = avo_TRAIN.Happiness\n",
    "X.head()\n",
    "#y.head()\n",
    "# Regression Time!\n",
    "# Sci Kit Learn! Scientific Computing library for python\n",
    "from sklearn import *\n",
    "# Pick the regression model we want to use\n",
    "\n",
    "# Decision Tree Regression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "# model = tree.DecisionTreeRegressor(random_state=2020)\n",
    "\n",
    "# Random Forest Regression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "model = ensemble.RandomForestRegressor(random_state=2024, n_estimators=200)\n",
    "# Split training into some for training and some for testing\n",
    "Xtrain, Xtest, ytrain, ytest = model_selection.train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print(\"training split: \", len(Xtrain), \"; test split: \", len(Xtest))\n",
    "# Perform regression on the data\n",
    "model.fit(X, y)\n",
    "model.score(Xtest, ytest)\n",
    "# Predict for Testing data\n",
    "# Clean Data\n",
    "\n",
    "\n",
    "# Read more advanced ways to clean data\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "# https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn\n",
    "# Select features\n",
    "TEST = pd.read_csv('dataset(test).csv', index_col='ID')\n",
    "\n",
    "XforPredictions = TEST[features]\n",
    "# Make Predictions!\n",
    "yPredictions = model.predict(XforPredictions)\n",
    "print(\"Predictions:\", yPredictions)\n",
    "# Save to CSV File!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but LinearDiscriminantAnalysis was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([70, 70, 65, 80, 85, 70, 70, 70, 80, 80, 80], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitting\n",
    "\n",
    "TEST = pd.read_csv('dataset(test).csv', index_col='ID')\n",
    "\n",
    "features = [\"Population\",\"Land size\",\"Working hours\",\"Corruption\",\"Environment\",\"Income\",\"Cost of living\",\"N recreational buildings\",\"N office buildings\",\"N essential buildings\",\"N residential buildings\"]\n",
    "XforPredictions = TEST[features]\n",
    "\n",
    "fitting.main(XforPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22300.0\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -80.750000\n",
      "         Iterations: 0\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\evanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\evanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\evanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\evanh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: -80.75\n",
       "        x: [ 5.000e+01  5.000e+01  5.000e+01  5.000e+01]\n",
       "      nit: 0\n",
       "      jac: [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n",
       " hess_inv: [[1 0 0 0]\n",
       "            [0 1 0 0]\n",
       "            [0 0 1 0]\n",
       "            [0 0 0 1]]\n",
       "     nfev: 5\n",
       "     njev: 1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import fitting\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    import numpy as np\n",
    "\n",
    "    #offset = np.random.normal(0, 10, 4)\n",
    "    #parameters = np.array([x1,x2,x3,x4]) + offset\n",
    "    \n",
    "    c1, c2, c3, c4, c5, c6, c7 = CONSTANTS\n",
    "    x1, x2, x3, x4 = x\n",
    "    return -1*MODEL.predict(np.array([c1, c2, c3, c4, c5, c6, c7, x1, x2, x3, x4]).reshape(1,-1))\n",
    "\n",
    "\n",
    "global CONSTANTS\n",
    "CONSTANTS = [2.7,223,31.6,2,6,96000,83600]\n",
    "\n",
    "global MODEL\n",
    "MODEL = fitting.main()\n",
    "\n",
    "\n",
    "buildings_max = CONSTANTS[1]/0.01\n",
    "print(buildings_max)\n",
    "\n",
    "#cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
    "#    {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
    "#    {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
    "#    {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
    "cons = ()\n",
    "    \n",
    "bnds = ((1, buildings_max), (1, buildings_max), (1, buildings_max), (1, buildings_max))\n",
    "\n",
    "#minimize(f, [50, 50, 50, 50], bounds=bnds, constraints=cons, method='Nelder-Mead')\n",
    "minimize(f, [50, 50, 50, 50], method='BFGS', options={'gtol': 1e-100, 'disp': True})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
